---
title: "Data 605 - Week 11 Discussion Board"
author: "Leticia Salazar"
date: "4/5/2022"
output:
  html_document:
    theme: flatly
    highlight: pygments
    toc: yes
    toc_float: yes  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

$~$

### Regression

$~$

#### Using R, build a  regression model for data that interests you. Conduct residual analysis.  Was the linear model appropriate? Why or why not?

```{r, message = FALSE, warning = FALSE}
# Load Libraries
library(openintro)
library(ggplot2)
library(tidyverse)
```


```{r}
# To check what additional datasets are available from the OpenIntro 
#package run the following code:

#data(package = "openintro")
```

$~$

#### For this assignment we will be using the gpa_study_hours dataset. The columns represent the variables gpa and study_hours for a sample of 193 undergraduate students who took an introductory statistics course in 2012 at a private US university. GPA ranges from 0 to 4 points, however one student reported a GPA > 4. This is a data error but this observation has been left in the dataset as it is used to illustrate issues with real survey data. Both variables are self reported, hence may not be accurate.

$~$

```{r}
# Loading dataset
data('gpa_study_hours', package = 'openintro') 
```

$~$

### Description of Data Set

$~$

#### A data frame with 193 rows and 2 columns:


|                          |                                            |
| ------------------------ | -------------------------------------------| 
| gpa                      | Grade point average (GPA) of student.      |
| study_hours              | Number of hours students study per week.   |


$~$

#### Let's checkout the data set

```{r}
# glimpse of data
glimpse(gpa_study_hours)
```

```{r}
# summary of data
summary(gpa_study_hours)
```


```{r}
gpa_study_hours %>%
ggplot(aes(x = gpa, y = study_hours)) +
    geom_point() +
    xlab("GPA") +
    ylab("Hours Studying") +
    ggtitle("Intro to Stats Class")
```


```{r}
# Creating a box plot to visualize potential outliers in data
boxplot(gpa~study_hours, data = gpa_study_hours, main = "Intro to Stats Class",
   xlab = "GPA", ylab = "Study Hours")
```

#### Note the 3 outliers in the box plot

$~$

```{r}
# Obtaining our Linear Model
my_lm <- lm(gpa_study_hours$gpa ~ gpa_study_hours$study_hours)
summary(my_lm)
```


```{r}
# Plotting gpa and study_hours with lm line
ggplot(gpa_study_hours, aes(gpa, study_hours)) +
  geom_point() +
  stat_smooth(method = lm, se = TRUE) +
  geom_segment(aes(xend = gpa, yend = study_hours), color = "blue", size = 0.3)
```


```{r}
hist(resid(my_lm), xlab = "", main = "Histogram of Residuals")
```


### Regression Diagnostics plots
```{r}
par(mfrow = c(1, 2))
plot(my_lm)
```

$~$

#### The diagnostic plots show residuals in four different ways:

##### Residuals vs Fitted. Used to check the linear relationship assumptions. A horizontal line, without distinct patterns is an indication for a linear relationship, what is good.

##### Normal Q-Q. Used to examine whether the residuals are normally distributed. Itâ€™s good if residuals points follow the straight dashed line.

##### Scale-Location (or Spread-Location). Used to check the homogeneity of variance of the residuals (homoscedasticity). Horizontal line with equally spread points is a good indication of homoscedasticity. This is not the case in our example, where we have a heteroscedasticity problem.

##### Residuals vs Leverage. Used to identify influential cases, that is extreme values that might influence the regression results when included or excluded from the analysis. This plot will be described further in the next sections.

$~$

### Conclusion:

$~$

#### There are four assumptions associated with a linear regression model:

##### **Linearity Assumption:**
##### Based on the Residuals vs. Fitted plot, the red line is approximately at zero, suggesting linearity. It also shows there is somewhat of a linear relationship between the two variables. 

##### **Homogeneity of Variance Assumption:**
##### Viewing the Scale-Location plot I see that there's not much of a variability of the residual points, instead the line seems to be decreasing suggesting heteroscedasticity.

##### **Normality of Residuals:**
##### Based on the QQ plot, it looks like the some points deviate from the line therefore, both are approximately normally distributed.

##### **Outliers and high Leverage Points Assumption:** The presence of outliers can affect the interpretation of the model and the high leverage points is present in a data of it had extreme predictor x values.
##### Before examing the Residuals vs. Leverage plot we were made aware that there was an error where a student reported a GPA > 4 and was left in the data. We can see Cook's distance lines and observation # 109 is close to the border but there are not influential points in our model.

$~$

#### Was the linear model appropriate? Why or why not?
* #### Based on the models I say yes, it was appropriate.

$~$

### References:

* https://www.openintro.org/data/
  * https://www.openintro.org/data/index.php?data=births14
  
* https://www.analyticsvidhya.com/blog/2016/07/deeper-regression-analysis-assumptions-plots-solutions/

* http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/#linearity-of-the-data

$~$

### Professor Fulton's Code:

```{r, warning = FALSE, message = FALSE}
require(car)
require(lmtest)
require(Hmisc)
require(ResourceSelection)

#sample data
enroll = c(14.55,22.18,61.69,38.02,89.29,67.57,49.16,43.74,26.37,24.74,11.57,19.61)
expend = c(49.53,64.05,138.08,341.35,234.38,164.16,244.08,117.93,110.89,57.74,72.88,89.08)
rwp = c(8.02,9.19,23.32,108.18,45.11,15.35,55.24,17.33,9.59,12.79,7.84,13.26)

#specify model, data, and level
model = expend ~ enroll + rwp
data = cbind(enroll, expend, rwp)
level = .95

#function
regressit = function(model, data, level) #model is the functional relationship among variables, , l is level, forecast is vector or matrix to forecast
    {
    kdepairs(data) #scatterplot
    myreg = lm(model, y = TRUE)    #runregression
    confint(myreg, level = level) #confidenceintervals
    r1 = summary(myreg) #regression summary
    r2 = anova(myreg) #ANOVA tableforregression
    r3 = coefficients(myreg) #coefficients
    r4 = myreg$residuals #residuals
    r5 = shapiro.test(r4) #test for normality of residuals
    r6 = bptest(myreg)  #Breusch-Pagan-Godfrey-Test for homoscedasticity, lmtest
    r7a = ncvTest(myreg) #non-constant variance test for homoscedasticity
    r7 = dwtest(myreg)  #independence of residuals  #test for independence of errors
    r8 = vif(myreg)  #look at collinearity, car package
    me = mean(myreg$residuals)
    mad = mean(abs(myreg$residuals))
    mse = mean(myreg$residuals^2)
    rmse = sqrt(mse)
    mpe = 100*mean(myreg$residuals / myreg$y)
    mape = 100*mean(abs(myreg$residuals) / myreg$y)
    aic = AIC(myreg)
    bic = BIC(myreg)
    all = c(me, mad, rmse, mpe, mape, aic, bic)
    names(all) = c("ME","MAD","RMSE","MPE","MAPE", "AIC","BIC")
    mybarplot = barplot(all)
    mylist = list(r1,r2,r3,r4,r5,r6,r7,r7a,r8,all)
    return(mylist)
    }

regressit(model,data,level)
```

